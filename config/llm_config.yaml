# TechCoach LLM Configuration
# Simple single-provider configuration using LangChain

# Choose one provider: kimi, openai, claude, gemini, deepseek
provider: "deepseek"

# Provider-specific configuration
providers:
  kimi:
    api_key: null  # Set your Kimi API key here or use KIMI_API_KEY env var
    model: "kimi-k2-0711-preview"
    api_base: "https://api.moonshot.cn/v1"
    
  openai:
    api_key: null  # Set your OpenAI API key here or use OPENAI_API_KEY env var
    model: "gpt-4o-mini"
    api_base: "https://api.openai.com/v1"
    
  claude:
    api_key: null  # Set your Claude API key here or use ANTHROPIC_API_KEY env var
    model: "claude-3-5-sonnet-20241022"
    api_base: "https://api.anthropic.com/v1/messages"
    
  gemini:
    api_key: null  # Set your Gemini API key here or use GEMINI_API_KEY env var
    model: "gemini-pro"
    api_base: "https://generativelanguage.googleapis.com/v1beta"
    
  # deepseek:
  #   api_key: null  # Set your DeepSeek API key here or use DEEPSEEK_API_KEY env var
  #   model: "deepseek-chat"
  #   api_base: "https://api.deepseek.com/v1"
  
  deepseek:  # by siliconflow
    api_key: null  # Set your DeepSeek API key here or use DEEPSEEK_API_KEY env var
    model: "Pro/deepseek-ai/DeepSeek-R1"          # limitation: tool call param misunderstanding
    # model: "Qwen/Qwen3-235B-A22B-Instruct-2507"  # limit by TPM 10000
    # model: "Pro/moonshotai/Kimi-K2-Instruct"
    # model: "Pro/deepseek-ai/DeepSeek-V3"
    api_base: "https://api.siliconflow.cn/v1"    

# Model settings
temperature: 0.5
max_tokens: 16384